{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483e94b4f8a8ad3d",
   "metadata": {},
   "source": [
    "Main content is currently provided solely as interactive life discussion.\n",
    "We look at a few of more commonly used mathematical concepts in Data Science, such as vector spaces, probability & distributions, frequency domain, and differential calculus. Specifically, we look at those form the perspective of Data Science, and how they can be used in practice to and how we can think of them in a more intuitive way.\n",
    "\n",
    "This notebook is primarily for running the visualizations and the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed382ed9a419f1",
   "metadata": {},
   "source": [
    "To keep track of the topics suggested for the discussion, in this notebook is also given this list of topics:\n",
    "\n",
    "- statistical distributions\n",
    "    - distributions: uniform, normal, etc; \n",
    "    - metrics: mean std mode, momenta space, AuC = 1\n",
    "    - distributions proximity: Entropy, KL distance, Wasserstein distance\n",
    "    - likelihood\n",
    "    - number of samples in the dataset and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b01196bc43ac68",
   "metadata": {},
   "source": [
    "# Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:14:29.384986Z",
     "start_time": "2025-08-08T15:14:29.365500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import simpson as simps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bd1294f6f151f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T14:50:41.435552Z",
     "start_time": "2025-08-08T14:50:41.422043Z"
    }
   },
   "outputs": [],
   "source": [
    "def plt_2_hist(data, bins=30, label=None, title=None, density=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 3), ncols=2, nrows=1)\n",
    "    ax[0].hist(data, bins=bins, alpha=0.5, density=density)\n",
    "    ax[1].hist(data, bins=bins, alpha=0.5, density=density, log=True)\n",
    "    \n",
    "    if label:\n",
    "        ax[0].set_xlabel(label)\n",
    "        ax[1].set_xlabel(label)\n",
    "    y_lbl = 'Density' if density else 'count'\n",
    "    \n",
    "    ax[0].set_title('Linear Scale')\n",
    "    ax[1].set_title('Log Scale')\n",
    "    ax[0].set_ylabel(y_lbl)\n",
    "    ax[1].set_ylabel(y_lbl)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(w_pad=0.6)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67660351f56970b",
   "metadata": {},
   "source": [
    "# Uniform Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266cd35d4b71a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:16:51.548156Z",
     "start_time": "2025-08-08T12:16:50.763760Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "uniform_distribution_dice = np.random.randint(1, 7, n_samples)\n",
    "plt_2_hist(uniform_distribution_dice, bins=6,\n",
    "           label='Dice outcome', title='Uniform Distribution of Dice Rolls', density=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22955d766ac54076",
   "metadata": {},
   "source": [
    "# Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2515fb0d586d3f1",
   "metadata": {},
   "source": [
    "1D darts residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10542dcf04faa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:17:06.488007Z",
     "start_time": "2025-08-08T12:17:02.062567Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 100_000_000\n",
    "normal_distribution_darts = np.random.normal(0, 1, n_samples)\n",
    "plt_2_hist(normal_distribution_darts, bins=30, label='Offset', title='Normal Distribution of Darts Residuals', density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162107351f7e5c",
   "metadata": {},
   "source": [
    "# derived values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c81abbde94dbd",
   "metadata": {},
   "source": [
    "# n-dimensional normal distribution, distance from the center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736b3182fb19c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:35:44.828523Z",
     "start_time": "2025-08-08T12:35:41.185331Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dims = list(range(1, 33, 1))\n",
    "sample_size = 1000\n",
    "\n",
    "mean_lengths = []\n",
    "std_lengths = []\n",
    "\n",
    "all_lengths = []\n",
    "for i, n in enumerate(dims):\n",
    "    X = np.random.randn(sample_size, n)  # 100,000 points in R^n\n",
    "    lengths = np.linalg.norm(X, axis=1)\n",
    "    mean_lengths.append(np.mean(lengths))\n",
    "    std_lengths.append(np.std(lengths))\n",
    "    if i % 10 == 0:\n",
    "        plt_2_hist(lengths, bins=100, label=f'n={n}', title=f'Distribution of Lengths of vectors in R^{n} sampled from N(0,1)', density=True)\n",
    "    all_lengths.append(lengths)\n",
    "    \n",
    "# Plot mean and 1 std dev as a function of n\n",
    "plt.plot(dims, mean_lengths, 'o-', label='Mean length', color='red')\n",
    "plt.plot(dims, np.array(mean_lengths) + np.array(std_lengths), 'r--', alpha=0.5, label='Mean Â± std')\n",
    "plt.plot(dims, np.array(mean_lengths) - np.array(std_lengths), 'r--', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Dimension $n$')\n",
    "plt.ylabel('Vector length')\n",
    "plt.title('Distribution of Lengths of 100,000 N(0,1) Vectors in $\\mathbb{R}^n$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba97f8798d506d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:35:48.449284Z",
     "start_time": "2025-08-08T12:35:47.659899Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bonus:\n",
    "\n",
    "# make t-test and the Z-test for all lengths wrt the last one, \n",
    "from scipy import stats\n",
    "tz_test_results = []\n",
    "\n",
    "for i, lengths in enumerate(all_lengths[:-1]):\n",
    "    t_stat, p_value = stats.ttest_ind(lengths, all_lengths[i+1], equal_var=False)\n",
    "    z_stat = (np.mean(lengths) - np.mean(all_lengths[i+1])) / (np.std(lengths) / np.sqrt(len(lengths)))\n",
    "    tz_test_results.append((dims[i], t_stat, p_value, z_stat))\n",
    "    \n",
    "# plot t-test and z-test results vs dims\n",
    "fig, ax = plt.subplots(figsize=(15, 4), ncols=3, nrows=1)\n",
    "\n",
    "all_dims, all_t_stats, all_p_values, all_z_stats = zip(*tz_test_results)\n",
    "ax[0].plot(all_dims, all_t_stats, 'o-', label='t-statistic', color='blue')\n",
    "ax[0].set_title('t-statistic vs Dimension')\n",
    "ax[0].set_xlabel('Dimension $n$')\n",
    "ax[0].set_ylabel('t-statistic')\n",
    "ax[0].grid(True)\n",
    "ax[1].plot(all_dims, all_p_values, 'o-', label='p-value', color='green')\n",
    "ax[1].set_title('p-value vs Dimension')\n",
    "ax[1].set_xlabel('Dimension $n$')\n",
    "ax[1].set_ylabel('p-value')\n",
    "ax[1].grid(True)\n",
    "ax[2].plot(all_dims, all_z_stats, 'o-', label='z-statistic', color='orange')\n",
    "ax[2].set_title('z-statistic vs Dimension')\n",
    "ax[2].set_xlabel('Dimension $n$')\n",
    "ax[2].set_ylabel('z-statistic')\n",
    "ax[2].grid(True)\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752f4503d649303",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c1cce7e322f17",
   "metadata": {},
   "source": [
    "Entropy is a measure of uncertainty or randomness in a distribution. For a discrete distribution, the entropy $H$ is defined as:\n",
    "$$H(X) = -\\sum_{i} p(x_i) \\log(p(x_i)) $$\n",
    "where $ p(x_i) $ is the probability of outcome $ x_i $.\n",
    "\n",
    "Low entropy says that the distribution is more predictable, while high entropy indicates more uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285523046d3d404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:42:14.363656Z",
     "start_time": "2025-08-08T12:42:14.346143Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy_dicrete(distribution):\n",
    "    \"\"\"Calculate the entropy of a discrete distribution.\"\"\"\n",
    "    probabilities = np.bincount(distribution) / len(distribution)\n",
    "    probabilities = probabilities[probabilities > 0]  # Remove zero probabilities\n",
    "    return -np.sum(probabilities * np.log2(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19043297dc37ab26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T12:45:49.058058Z",
     "start_time": "2025-08-08T12:45:49.045059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Entropy of a uniform distribution\n",
    "n_samples = 1000\n",
    "uniform_distribution_dice = np.random.randint(1, 7, n_samples)\n",
    "uniform_distribution_coin = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "unfair_distribution_coin = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])\n",
    "\n",
    "print(\"Entropy of fait distribution (dice):\", entropy_dicrete(uniform_distribution_dice))\n",
    "print(\"Entropy of fait distribution (coin):\", entropy_dicrete(uniform_distribution_coin))\n",
    "print(\"Entropy of unfair distribution (coin):\", entropy_dicrete(unfair_distribution_coin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213434948ae0ccd",
   "metadata": {},
   "source": [
    "How many samples do we need to detect an unfair coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ee702685af8af",
   "metadata": {},
   "source": [
    "# Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948a485ff1c7d28",
   "metadata": {},
   "source": [
    "Cross-entropy measures the difference between two probability distributions. It is defined as:\n",
    "$$H(p, q) = -\\sum_{i} p(x_i) \\log(q(x_i))$$\n",
    "\n",
    "where $p$ is the true distribution and $q$ is the estimated distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff4257f48c8902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:19:43.224947Z",
     "start_time": "2025-08-08T15:19:43.200314Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_kde(p, q, n_points=1000):\n",
    "    # Estimate PDFs\n",
    "    p_kde = gaussian_kde(p)\n",
    "    q_kde = gaussian_kde(q)\n",
    "    # Evaluate on grid\n",
    "    grid = np.linspace(min(p.min(), q.min()), max(p.max(), q.max()), n_points)\n",
    "    p_pdf = p_kde(grid)\n",
    "    q_pdf = q_kde(grid)\n",
    "    # Avoid log(0)\n",
    "    q_pdf = np.where(q_pdf == 0, 1e-12, q_pdf)\n",
    "    # Cross-entropy estimate\n",
    "    ce = -np.sum(p_pdf * np.log(q_pdf)) * (grid[1] - grid[0])\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_wasserstein_distance_kde(p, q, n_points=1000):\n",
    "    \"\"\"\n",
    "    Estimate the 1D Wasserstein distance between distributions given by samples p and q,\n",
    "    using KDE-smoothed CDFs.\n",
    "    \"\"\"\n",
    "    p_kde = gaussian_kde(p)\n",
    "    q_kde = gaussian_kde(q)\n",
    "    # Evaluate on grid\n",
    "    grid = np.linspace(min(p.min(), q.min()), max(p.max(), q.max()), n_points)\n",
    "    p_pdf = p_kde(grid)\n",
    "    q_pdf = q_kde(grid)\n",
    "    # Avoid log(0)\n",
    "    q_pdf = np.where(q_pdf == 0, 1e-12, q_pdf)\n",
    "    # Cross-entropy estimate\n",
    "    ce = -np.sum(p_pdf * np.log(q_pdf)) * (grid[1] - grid[0])\n",
    "    \n",
    "    cdf_p = np.cumsum(p_pdf)\n",
    "    cdf_p /= cdf_p[-1]\n",
    "    cdf_q = np.cumsum(q_pdf)\n",
    "    cdf_q /= cdf_q[-1]\n",
    "    wasserstein = simps(np.abs(cdf_p - cdf_q), x=grid)\n",
    "    \n",
    "    return ce, wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74028cc7ee9171ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:15:59.318650Z",
     "start_time": "2025-08-08T15:15:51.146065Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 100_000\n",
    "normal_distribution_darts_1 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "normal_distribution_darts_2 = np.random.normal(0, 0.1, n_samples)\n",
    "print(\"Cross-entropy between two normal distributions:\", cross_entropy_kde(normal_distribution_darts_1, normal_distribution_darts_2, n_points=1000))\n",
    "print(\"Wasserstein distance between two normal distributions:\", cross_entropy_wasserstein_distance_kde(normal_distribution_darts_1, normal_distribution_darts_2, n_points=1000)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befb243b25c9672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:33:03.347431Z",
     "start_time": "2025-08-08T15:22:13.876169Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 10_000\n",
    "normal_distribution_darts_1 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "entropy_values = []\n",
    "wd = []\n",
    "stds = np.logspace(-2, 2, 21)  # Logarithmic scale for better visualization\n",
    "for s in stds:\n",
    "    normal_distribution_darts_2 = np.random.normal(0, s, n_samples)\n",
    "    ev = cross_entropy_kde(normal_distribution_darts_1, normal_distribution_darts_2, n_points=10_000)\n",
    "    #ev, w = cross_entropy_wasserstein_distance_kde(normal_distribution_darts_1, normal_distribution_darts_2, n_points=10_000)\n",
    "    print(f\"Cross-entropy between N(0, 1) and N(0, {s}):\", ev) #, \"WD:\", w)\n",
    "    entropy_values.append(ev)\n",
    "    #wd.append(w)\n",
    "    \n",
    "plt.semilogx(stds, entropy_values, 'o-')\n",
    "plt.xlabel('Standard Deviation of q N(0, s)')\n",
    "plt.ylabel('Cross-Entropy')\n",
    "plt.show()\n",
    "\n",
    "#plt.semilogx(stds, wd, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61844a26729a481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:11:48.038877Z",
     "start_time": "2025-08-08T15:07:36.407168Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_distribution_darts_1 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "entropy_values = []\n",
    "ms = np.linspace(-20, 20, 21)\n",
    "for m in ms:\n",
    "    normal_distribution_darts_2 = np.random.normal(m, 1, n_samples)\n",
    "    #plt.hist(normal_distribution_darts_1, bins=30, alpha=0.5, density=True, label=f'N({m}, 1)')\n",
    "    # plt.hist(normal_distribution_darts_2, bins=30, alpha=0.5, density=True, label=f'N({m}, 1)')\n",
    "    # plt.show()\n",
    "    \n",
    "    ev = cross_entropy_kde(normal_distribution_darts_1, normal_distribution_darts_2, n_points=10_000)\n",
    "    print(f\"Cross-entropy between N(0, 1) and N({m}, 1):\", ev)\n",
    "    entropy_values.append(ev)\n",
    "    \n",
    "plt.plot(ms, entropy_values, 'o-')\n",
    "plt.xlabel('Mean of q N(m, 1)')\n",
    "plt.ylabel('Cross-Entropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b3db6d53ac50e",
   "metadata": {},
   "source": [
    "exercise - compare cross-entropy and Wasserstein distance for different distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb7fc90912ea06",
   "metadata": {},
   "source": [
    "# likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a45e8c48ebba6e",
   "metadata": {},
   "source": [
    "Probability = given model $P(X | \\theta)$, where $X$ is the data and $\\theta$ is the model parameter. How probable is the sample given the model's parameters $\\theta$?\n",
    "\n",
    "Likelihood is not a probability distribution, but a function of the parameters $\\theta$ given the data $X$. It says how likely the parameters are given the data.\n",
    "\n",
    "Here let's consider the model of a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. The likelihood function is:\n",
    "\n",
    "$$L(\\mu, \\sigma | X) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where $x_i$ are the data points.\n",
    "\n",
    "This is difficult to maximize, so we often use the log-likelihood. Since the logarithm is a monotonic function, maximizing the log-likelihood is equivalent to maximizing the likelihood, and equivalent to minimizing the negative log-likelihood:\n",
    "\n",
    "$$\\mu, \\sigma = \\arg\\max_{\\mu, \\sigma} L(\\mu, \\sigma | X) = $$\n",
    "$$\\arg\\max_{\\mu, \\sigma} \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) = $$\n",
    "$$\\arg\\max_{\\mu, \\sigma} \\sum_{i=1}^{n} \\left(-\\log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) = $$\n",
    "$$\\arg\\min_{\\mu, \\sigma} \\left(\\frac{n}{2}\\log(2\\pi) + n\\log(\\sigma) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
